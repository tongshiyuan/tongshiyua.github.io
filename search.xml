<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>WGS分析笔记（2）-- bwa vs bowtie2</title>
      <link href="/2019/06/18/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89-%20bwa%20vs%20bowtie2/"/>
      <url>/2019/06/18/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89-%20bwa%20vs%20bowtie2/</url>
      
        <content type="html"><![CDATA[<ul><li><p>在进行正式的mapping记录之前，先记录一下bwa与bowtie2在mapping这个环节的情况。<br></p></li><li><p>一般对于WGS结果的mapping，一般推荐的软件有两款，分别是bwa和bowtie2，大多数的公司报告或者网上的教程，我所看到的都是使用bwa进行比对的，这里，我来进行一下两个软件的对比。<br></p></li><li><p>实验对象还是之前文章提到的那个样本的数据，我只取用其中的一对数据进行mapping并比较。<br></p></li><li><p>比较之前先进行一下软件安装、参考序列下载并建立索引<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bowtie2</span><br><span class="line">$ wget https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4.3/bowtie2-2.3.4.3-linux-x86_64.zip</span><br><span class="line"># https://sourceforge.net/projects/bowtie-bio/files/bowtie2/2.3.4/bowtie2-2.3.4-linux-x86_64.zip</span><br><span class="line">$ unzip bowtie2-2.2.9-linux-x86_64.zip</span><br><span class="line">$ ln -sf /biosoft/bowtie2-2.3.4.3-linux-x86_64/bowtie2 /home/shiyuantong/bin/bowtie2</span><br><span class="line">BWA:</span><br><span class="line">$ wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2</span><br><span class="line">$ tar -jxvf bwa-0.7.17.tar.bz2 # x extracts, v is verbose (details of what it is doing), f skips prompting for each individual file, and j tells it to unzip .bz2 files</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure></li><li><p>关于安装这里有一些需要注意的地方！！！！！<br></p></li><li><p>首先是bowtie2，建议大家使用2.3.4的下载链接，我在下载的时候最新版是2.3.4.3，但是在使用的出现了报错！！！！！<br></p></li><li><p>报错的内容如下（当时没截图）：<br><br>&emsp;&emsp;&emsp;&emsp;Segmentation fault (core dumped) (ERR): bowtie2-align exited with value 139</p></li><li><p>这个报错只会出现在批量处理的脚本中，对单个样本的处理并没有影响，但是实际使用的时候，大家都是批量处理样本，怎么可能一个样本一个命令，因此推荐2.3.4的版本，当然，下面的比较不会涉及这个问题。<br></p></li><li><p>还有就是BWA了，这个软件ubuntu用户也可以直接使用<code>sudo apt-get install bwa</code>命令进行安装，我看了一下，两种方法的版本是一致的，都是0.7.17。<br></p></li><li><p>然后是参考序列，这里直接使用ucsc的hg19，下载与建立索引方式如下，根据自己的需要调整目录<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">hg19：</span><br><span class="line">$ cd /your/path/of/reference/</span><br><span class="line">$ wget http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz</span><br><span class="line">$ tar zvfx chromFa.tar.gz</span><br><span class="line">$ cat *.fa &gt; hg19.fa</span><br><span class="line">$ rm chr*.fa</span><br><span class="line">建立bwa索引：</span><br><span class="line">$ bwa index -a bwtsw  hg19.fa</span><br><span class="line"> # 产生.bwt .pac .ann .amb .sa五个新文件</span><br><span class="line"># -a：两种构建index算法，bwtsw以及is，bwtsw适用大于10MB的参考基因组，比如人，is适用于小于2GB的数据库，是默认的算法，速度较快，需要较大的内存，</span><br><span class="line"># -p：输出数据库的前缀，默认与输入文件名一致，这里我没有加这个参数，直接输出到当前目录</span><br><span class="line">建立bowtie2索引：</span><br><span class="line">$ bowtie2-build hg19.fa hg19.fa</span><br><span class="line"># bowtie2-build命令在安装bowtie2的目录下找到</span><br><span class="line"># 第一个hg19.fa代表输入的参考序列</span><br><span class="line"># 第二个hg19.fa代表输出的索引文件前缀</span><br><span class="line"># 产生六个.bt2新文件</span><br></pre></td></tr></table></figure></li><li><p>上述程序建立索引速度较慢，尤其是bowtie2，但是一次建好，一劳永逸，请大家耐心等待，也可以放在后台，防止终端突然的中断。<br></p></li><li><p>建立好索引就可以直接开始比对了，以下是我的比对程序，都开了24线程，用<code>nohup …… &amp;</code>放在后台运行，用<code>time</code>记录时间。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ nohup time bowtie2 -p 24 -x /your/path/of/reference/ucsc.hg19.fasta --rg-id W2018001 --rg PL:ILLUMINA --rg LB:W2018001 --rg SM:W2018001 -1 W2018001_NZTD180602206_HCV5MDMXX_L1.cleaned.1.fq.gz -2 W2018001_NZTD180602206_HCV5MDMXX_L1.cleaned.2.fq.gz -S W2018001.bowtie2.sam &gt; W2018001.bowtie2.log &amp;</span><br><span class="line">$ nohup time bwa mem -t 24 -M -R &quot;@RG\tID:W2018001\tPL:ILLUMINA\tLB:W2018001\tSM:W2018001&quot; /your/path/of/reference/ucsc.hg19.fasta W2018001_NZTD180602206_HCV5MDMXX_L1.cleaned.1.fq.gz W2018001_NZTD180602206_HCV5MDMXX_L1.cleaned.2.fq.gz 1&gt;W2018001.bwa.sam 2&gt;W2018001.bwa.log &amp;</span><br></pre></td></tr></table></figure></li><li><p>这一步会比较久，我也是经过漫长的半天等待终于迎来了结果，首先看一下速度吧。先前的脚本使用了<code>time</code>的命令，可以直接看到速度，在日志文件里。<br><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/%E6%97%B6%E9%97%B4%E5%AF%B9%E6%AF%94.png" alt="时间对比"></p></li><li><p>日志文件的最后两行就是<code>time</code>命令输出的结果，所以没有必要用<code>cat</code>查看，而且bwa的日志文件，要是用<code>cat</code>怕是屏幕要炸。图中可以看到两个红色的框，就是我标出来的时间。（其实我原来用<code>time</code>命令，结果不长这样的，这个结果不太利于观看，但是也能说明问题了）<br></p></li><li><p>很明显的可以看到半套全基因组的数据（我只用了样本一半的数据）bowtie2跑的更快一些，但其实大家不用纠结这个点。因为上一次我用24线程，一样的脚本一样的数据，跑bowtie2花了六个多小时，速度没有bwa快，同时以前在使用酵母的测序数据（数据量会比较小）的时候，明显发现bwa速度比bowtie2快，甚至在说法上你也会发现不同的人给你的说法是不一样的，有些人说bwa快有些人说bowtie2快，网上看帖子也没有一个十分明确的说法哪个速度快。这里大家完全可以用自己的数据和脚本跑一下，看看结果。<br></p></li><li><p>接下来我想看看比对效果，这里我先采用了samtools的flagstat分别进行统计，下面是安装samtools的步骤：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">samtools:</span><br><span class="line">$ wget https://github.com/samtools/samtools/releases/download/1.9/samtools-1.9.tar.bz2</span><br><span class="line">$ tar xvfj samtools-1.9.tar.bz2</span><br><span class="line">$ cd samtools-1.9</span><br><span class="line">$ ./configure --prefix=/where/to/install</span><br><span class="line">$ make</span><br><span class="line">$ make install</span><br><span class="line"># samtools其实我到现在为止装的最崩溃的软件之一了，因为在实际安装的时候你会发现它需要各种各样的库的支持，对于使用新机器的我，我基本是安装，报错缺什么库，安装缺的库，重新安装，这么折腾了一下午</span><br></pre></td></tr></table></figure></li><li><p>接下来就是使用统计工具，其实很简单。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ samtools flagstat W2018001.bowtie2.sam &gt;W2018001.bowtie2.flagstat</span><br><span class="line">$ samtools flagstat W2018001.bwa.sam &gt;W2018001.bwa.flagstat</span><br></pre></td></tr></table></figure></li><li><p>这个也需要花一点时间，但不会太长，看一下结果。<br><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/flagstat.png" alt="flagstat"></p></li><li><p>这个结果还是比较清楚的，bwa的结果比bowtie2稍稍好一点。但相差不是很大，所以对于这两个软件，一直是公说公有理，婆说婆有理，这里我用另一个软件对结果进行统计，再进行对比试试。<br></p></li><li><p><a href="https://rseqc.sourceforge.net" target="_blank" rel="noopener">RSeQC</a>是一个功能强大的软件，里面有很多实用的小工具，其中的bam_stat就是一个实用的bam/sam结果统计工具，安装方式也是相当简单了，就是一个python的包，支持python2.x和python3.x，这里我选用python3的pip来安装，因为本人习惯使用python3。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install RSeQC</span><br></pre></td></tr></table></figure></li><li><p>使用和结果如下，由于我这个sam文件比较大，运行起来比较慢，所以我开了俩终端。<br><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bam_stat.png" alt="bam_stat"></p></li><li><p>其实我最想看的unique mapping的reads，因为后期为了降低假阳性，在处理bam文件的时候会选择unique mapped的reads，但是在查看说明书无果后，找遍论坛没有找到一个能够说服我的筛选unique mapped的方式。<br></p></li><li><p>有这么几个方式，一个说是看tag，但是bwa的结果，你仔细看说明书和结果，会发现，这个tag并没有什么用，bowtie2倒是还可以。<br></p></li><li><p>第二个也是说的比较多的一个，看MAPQ。那么mapq是啥呢，我来贴几张图。<br></p></li></ul><p><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bowtie2%E7%9A%84%E8%AF%B4%E6%98%8E.png" alt="bowtie2的说明"></p><p><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/SAM%E7%9A%84%E8%AF%B4%E6%98%8E.png" alt="SAM的说明"></p><p><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/%E5%AE%98%E7%BD%91%E8%AF%B4%E6%98%8E.png" alt="官网说明"></p><ul><li>分别是sam格式官网的说明，bowtie2官网的说明，这两个说明的公式是一样的，都指向最后一个官网的说明。看到这个官网的公式，我直接就傻掉了，反正到现在也没推出个所以然来。但是前两张图就很好理解了。但是和很多人说的MAPQ&gt;=1就是unique mapping，我觉得是对不上的。对于这点我不多说，<a href="http://qiubio.com/archives/3321" target="_blank" rel="noopener">这里</a>的解释也是目前为止我最能接受的。<br></li><li>那上面那个结果，bam_stat，我在阅读源码后，发现是以MAPQ&gt;=30作为阈值来挑选是否unique的。由于bwa和bowtie2的mapq的计算方式不一样，这个结果其实并不可信。于是我写了一个脚本，看了一下mapq的分布情况。<br></li></ul><p><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bowtie2.png" alt="bowtie2"></p><p><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bwa.png" alt="bwa"></p><ul><li><p>这个图能说明什么呢，有待商榷。<br></p></li><li><p>这个时候再回过来看一下bowtie2的输出结果和大家说的bowtie2的筛选unique mapping的方法以及结果。<br><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bowtie2.log.png" alt="bowtie2.log"><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/bowtie2_tag.png" alt="bowtie2_tag"></p></li><li><p>其实到这里我也不知道该怎么办了，到现在还是不知道，bwa结果用mapq&gt;=1是否能得到unique mapping。这个结果对于后续分析影响有多大我不好说，至于怎么选择，我也不发表意见，不过在后续的筛选中，gatk给出的建议是mapq&gt;=40。<br></p></li><li><p>2019-1-9补充bwa结果，按mapq分布，分别计算&gt;=1,10,20,30的比例。为大家选择MAPQ作为筛选提供一个参考。<br><br><img src="/2019/06/18/WGS分析笔记（2）- bwa vs bowtie2/mapq%E5%88%86%E5%B8%83.png" alt="MAPQ比例"></p></li><li><p>但是回归主题，bwa和bowtie2，我决定选择bwa。<br></p></li><li><p>水平有限，要是存在什么错误请评论指出，可发送邮件至<a href="mailto:shiyuant@outlook.com" target="_blank" rel="noopener">shiyuant@outlook.com</a>！请大家多多批评指正，相互交流，共同成长，谢谢！！！<br></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 我的WGS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WGS </tag>
            
            <tag> 生信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WGS分析笔记（1）- 原始数据以及质控</title>
      <link href="/2019/06/18/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89-%20%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E8%B4%A8%E6%8E%A7/"/>
      <url>/2019/06/18/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89-%20%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E8%B4%A8%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<ul><li><p>二代测序方式分为三种：<br></p><ul><li><em>single read 单端测序</em><br></li><li><em>paired-end read 双端测序</em><br></li><li><em>mate-pair read 配对测序</em><br></li></ul></li><li><p>虽然有三种方式，但是大多数数据（包括本课题）为双端测序，至于三者之间的差别、优缺点以及适用场合可以自行搜索。<br></p></li><li><p>接下来讲的内容都是基于双端测序的。<br></p></li><li><p>测序仪原始下机的数据我们称为raw data，二代测序是将DNA片段打断了再测的，每个测序片段我们称为read，质控完的数据称为clean data。既然是双端测序，那么文件就是成对出现的，分别记录reads两端的信息：一般的命名是*.fq1.gz、*.fq2.gz（’ * ‘ 表示通配符），这是一个fastq文件，通常以fq或fastq作为后缀，具体内容下方会介绍。这里我给大家展示我们课题的一个真实数据，给大家看看大小以及命名方式。<br><br><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E5%B1%95%E7%A4%BA.png" alt="原始数据展示"></p></li><li><p>我们可以看到，首先我把我的用户名打码了，呵呵，开个玩笑。<br></p></li><li><p>首先这个样本总共有五个文件，这就是公司给的原始数据，我原封不动的上传到了我的服务器上，也没改名也没怎样，我在图片上画了五个蓝色的框，下面是分别代表的意思：<br></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W2018001：是样本的名字，其实我一开始提交给公司的名字是2018001，为什么多了个W我也不知道啊。</span><br><span class="line">NZTD...：这一串是公司自动生成的编号，他们内部使用的，不知道也不需要知道啥意思，不同的公司，可能没有这个</span><br><span class="line">HCV...：flowcell_ID的信息，一般情况下一个样本是一样的</span><br><span class="line">L1：flowcell_lane的信息，图中有L1和L2，这也是为什么一个数据他给我四个文件的原因，他在不同的lane上测的，这里还要注意的一点就是，这个lane的值可能是同一个，就是即使碰到都是L1也是可能的</span><br><span class="line">1，2：分别代表reads的两端</span><br></pre></td></tr></table></figure><ul><li>我们可以看到，这个30X的WGS的数据量还是很大的，所以为什么他会是压缩文件，不同公司给的命名可能不一样，不过大体不会相差太多，具体命名的含义也可以问问公司的。<br></li><li>这里给到的是一个样本2对文件，实际情况是，可能有多对或者只有一对文件，对于这样的情况，我们的处理方式一般是看作一个文件处理，就是merge一下，有见过有些课题组是在这一步直接merge，我的处理方式是后期bam文件再merge。<br></li><li>好了，拿到这么一个数据，我们做的第一件事情应该是校验数据的完整性！！！这点很重要，即使一般情况下，传输都不会出错，但这一步还是要做的，怎么去做呢，就看这个截图里的第一个文件MD5.txt，其实这就是一个文本文件，打开了看看是这样的<br></li></ul><p><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/MD5%E4%BF%A1%E6%81%AF.png" alt="MD5信息"></p><ul><li>就是这样的四行信息，也就是一个文件名对应一个校验码，最简单的校验方式，linux系统自带的MD5校验命令：<br></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ md5sum -c MD5.txt</span><br></pre></td></tr></table></figure><ul><li>这个还是需要一点点时间的，大家要耐心等待。展示一下我这里的结果吧（其实在上传完数据的时候我就校验过）<br></li></ul><p><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/%E6%A0%A1%E9%AA%8C%E7%BB%93%E6%9E%9C.png" alt="校验结果"></p><hr><ul><li><p>好了，校验完了，没问题，那么就给大家介绍一下文件的格式。<br></p></li><li><p>文件是什么呢，其实就是文本文件，可以直接查看，以下是示例：<br><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/fastq%E6%A0%BC%E5%BC%8F.png" alt="fastq格式"></p></li><li><p>一条read一条记录，一条记录占四行，第一行是注释，第二行是序列，就是我们所说的ATCG碱基序列，第三行是‘+’，第四行是对应的每个碱基的测序质量，也就是fastq中的“q”。每条记录之间是没有空格的。<br></p></li><li><p>贴一下关于第一行的解释<br></p></li></ul><table><thead><tr><th>EAS139</th><th>The unique instrument name</th></tr></thead><tbody><tr><td>136</td><td>Run ID</td></tr><tr><td>FC706VJ</td><td>Flowcell ID</td></tr><tr><td>2</td><td>Flowcell lane</td></tr><tr><td>2104</td><td>Tile number within the flowcell lane</td></tr><tr><td>15343</td><td>‘x’-coordinate of the cluster within the tile</td></tr><tr><td>197393</td><td>‘y’-coordinate of the cluster within the tile</td></tr><tr><td>1</td><td>Member of a pair, 1 or 2 (paired-end or mate-pair reads only)</td></tr><tr><td>Y</td><td>Y if the read fails filter (read is bad), N otherwise</td></tr><tr><td>18</td><td>0 when none of the control bits are on, otherwise it is an even number</td></tr><tr><td>ATCACG</td><td>Index sequence</td></tr></tbody></table><ul><li><p>第一个是测序机器ID，独一无二的，你可以去illumina官网查到的（我没查过）<br></p></li><li><p>第二个是这个机器跑的次数，一般机器都是有使用寿命的，所以次数越多肯定结果越差，那么一般在什么区间合适呢，我的老师给的建议是200-9999，为啥还有下限呢，那是因为，机器刚开始跑的那几次，需要磨合嘛，不是很准。看来这个展示数据不是很好啊，吓得我赶紧去看了下我的数据，还好，在212次（懒得贴图了，hexo弄个图比简书还麻烦），在范围内。<br></p></li><li><p>倒数第三个，表示数据有没有被过滤过，一般我们的原始数据肯定是N的，要是给的原始数据是Y，你就得好好问问公司原因了。<br></p></li><li><p>其余具体的我就不解释了，有问题可以评论交流。<br></p></li><li><p>第二行要注意的是，可能有N的出现，那是因为有些碱基没被测出来。<br></p></li><li><p>第四行是ASCII码表示的碱基质量，这样就能保证质量是用一个字符表示的，和碱基一一对应。公式如下：<br><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;P=10^-[(n-33)/10]</p></li><li><p>举个栗子：比如“?”在ASCII码表上对应的编号是63，那么n就是63，减去33以后就是30，也就是我们说的Q30了，所以<code>Q = n - 33</code>，P算出来就是0.001，这个就是错误率，反过来，准确率就是99.9%，Q30就是准确率99.9%，同理Q20就是准确率99%。<br></p></li><li><p>ok，介绍完格式，介绍两款质控的软件，fastqc和fastp<br></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fastqc：</span><br><span class="line">$ wget http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.7.zip</span><br><span class="line">$ unzip fastqc_v0.11.7.zip</span><br><span class="line">$ cd FastQC/</span><br><span class="line">$ chmod 755 fastqc</span><br><span class="line">fastp：</span><br><span class="line">$ wget http://opengene.org/fastp/fastp</span><br><span class="line">$ chmod 755 fastp</span><br></pre></td></tr></table></figure><ul><li>以上是我的安装方式，fastqc是一个很常用的软件，我想只要不是小白都用过。对于ubuntu用户请用我提供的方式进行安装，用<code>apt-get install fastqc</code>可能在使用的时候出现如下报错<br></li></ul><p><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/fastqc%E9%94%99%E8%AF%AF.jpg" alt="fastqc错误"></p><ul><li>至于fastp，是用来清洗质量不好的reads的，其实类似的软件很多，包括trimmomatic等，之所以选择这个是因为用过那么多软件后，发现fastp无论在安装还是使用上都很顺手，仅此而已。<br></li><li>好了，现在让我们来使用fastqc对raw data的质量进行分析并查看<br></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN</span><br><span class="line">#这是fastqc的使用方法，其实很简单，对于不懂的软件，我一般的推荐是，先看官方说明，再逛逛论坛，这里不展开说这个软件怎么使用了，下面是我的使用代码</span><br><span class="line">$ fastqc -o your/path/to/out -t 4 $&#123;datadir&#125;/*.fq.gz</span><br><span class="line">#是不是超级简单，-o是报告输出的目录，这个目录必须是存在的，-t是线程数</span><br></pre></td></tr></table></figure><ul><li>结果如下，要放在早几年，估计是很难见到这么干净的原始数据了，为此我还专门请教了一些老师，这么干净的数据的可信程度。得益于illumina公司对仪器的升级改善，数据的质量也是越来越好了。<br></li></ul><p><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/fastqc.png" alt="fastqc.png"></p><ul><li><p>但是即便如此，我们还是要对raw data进行清洗的，数据的质量直接决定了后面分析的准确性，是一切分析的前提，我们一直都在说QC，QC也是基础，但是它的重要性不容忽视。下面是我要查看的质控的内容：<br></p><p>​    • read各个位置的碱基质量值分布<br></p><p>​    • 碱基的总体质量值分布<br></p><p>​    • read各个位置上碱基分布比例，目的是为了分析碱基的分离程度<br><br>​    • GC含量分布<br></p><p>​    • read各位置的N含量<br><br>​    • read是否还包含测序的接头序列<br></p></li><li><p>下面是我的对于clean data的指标：<br></p><p>​    1.将含有接头的reads对去除，或者cut接头<br></p><p>​    2.测序错误率分布检查，一般情况下，每个碱基位置的测序错误率都应该低于1%<br></p><p>​    3.GC含量，理论上A=T，C=G，前几个碱基可能会有波动，GC含量整体在41.5%左右<br><br>​    4.平均质量值分布：Q30平均比例≥80%，平均错误率≤0.1%<br></p></li><li><p>为了达到这样的目标，我的筛选是条件是：<br></p><p>​    去除含接头（adapter）的一对reads；可以酌情考虑去除接头保留reads<br></p><p>​    当某一端read中的N含量超过该条read长度比例10%，去除该对reads<br><br>​    当某一端read中低质量（Q≤20）碱基数超过该read长度比例的50%时，去除该对reads<br></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ fastp -i 1.fq.gz -I 2.fq.gz \</span><br><span class="line">--adapter_sequence $&#123;Adapter_R1&#125; \</span><br><span class="line">--adapter_sequence_r2 $&#123;Adapter_R2&#125; \#不同的试剂盒、仪器，会有不同的接头，这个可以咨询公司，也可以选择去掉这两个参数，影响不大</span><br><span class="line">-o your/path/of/data/cleaned.1.fq.gz \</span><br><span class="line">-O your/path/of/data/cleaned.2.fq.gz \#输出的clean data的位置和名称</span><br><span class="line">-c -q 20 -u 50 -n 15 -5 20 -3 20 -w 16 \</span><br><span class="line">-h your/path/of/report/clean.html -j your/path/of/report/clean.json #这俩参数分别是不同格式报告的输出位置和文件名</span><br></pre></td></tr></table></figure><ul><li><p>别的不多说，解释一下我用到的几个参数:<br></p><p>​    -c：对overlap区域进行纠错，适用于PE<br></p><p>​    -q：设置低质量的标准，默认是15，多数公司这里设为5<br><br>​    -u：低质量碱基所占比例，默认40代表40%，只要有一条read不满足条件就成对丢掉<br><br>​    -n：过滤N碱基过多的reads，15代表个数，因为一般PE150的reads长度是150<br><br>​    -w：线程数，这里要注意！范围是1-16，建议是8，亲测，8和16差不多<br><br>​    -5 -3：根据质量值来截取reads，分别对应5‘端和3’端，得到reads长度可能不等<br></p><p>具体请参考官网说明<br></p></li><li><p>这个时候再看clean data结果，就无需fastqc了，因为fastp也会生成一份报告。<br></p></li></ul><p><img src="/2019/06/18/WGS分析笔记（1）- 原始数据以及质控/fastp.png" alt="fastp报告部分截图"></p><ul><li>看到结果，其实可以发现，数据还是保留了大部分的，所以完全不用担心cut太狠，导致数据不够。我的原则是，在数据量面前，更应该保证的是数据的准确性，即使会有一点损失，也是值得的，毕竟research更注重的也是准确性，相对于降低假阴性，我更倾向于提高真阳性。<br></li><li>那么得到clean data以后就可以进行下一步mapping了，mapping内容下次再与大家分享。<br></li><li>水平有限，要是存在什么错误请评论指出，可发送邮箱至<a href="mailto:shiyuant@outlook.com" target="_blank" rel="noopener">shiyuant@outlook.com</a>！请大家多多批评指正，相互交流，共同成长，谢谢！！！<br></li></ul>]]></content>
      
      
      <categories>
          
          <category> 我的WGS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WGS </tag>
            
            <tag> 生信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WGS分析笔记（0）- 我的数据</title>
      <link href="/2019/06/17/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%880%EF%BC%89-%20%E6%88%91%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/06/17/WGS%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0%EF%BC%880%EF%BC%89-%20%E6%88%91%E7%9A%84%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<ul><li>我研究的课题主要是通过二代重测序获得基因组数据进而进行疾病分析<br></li></ul><hr><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li>二代测序技术（next generation sequencing，NGS），又称为高通量测序技术（high-throughput sequencing，HTS）或深度测序（deep sequencing），通过一次对几十万到几百万条核酸分子进行序列测定，进而对一个物种的转录组和基因组进行细致全貌的分析<br></li><li>一般我们对人类基因组的研究策略主要是三个：<br><ul><li>全基因组重测序（whole genome re-sequencing，WGS）<br></li><li>全外显子组重测序（whole exome re-sequencing，WES）<br></li><li>目标区域测序（target region re-sequencing，TR）<br></li></ul></li><li>对于这三个策略如何应用，这完全看课题需要，没有人说过这三个策略必须分开进行，实际上目前很多课题组在进行研究的时候，都会先分析WES和panel的数据，再考虑进行WGS的测序分析，也有些课题组通过panel筛选病例，再进行WES或WGS的分析。以上只是两个简单的思路，适用于不同的课题需求。那到底怎么去选取研究策略，还是根据课题的需求，这充分说明了，课题设计的重要性，从我踩过的坑来看，一个好的课题设计，能让你后续的分析少碰很多壁，而且我觉得课题是循序渐进的，具体怎么去做要根据预实验好好的验证<br></li><li>WGS和WES怎么选择呢，也是看研究疾病的情况，比较而言，WES的成本会低，而目前报道来看80%的疾病都是由于exon区域的突变贡献的，WGS的成本相对较高，深度也不及WES，类似嵌合体什么的，靠WGS是检测不出来的，但是WGS的覆盖广，noncoding区域一直是有待研究的区域，WGS也有利于SV的检测，目前大多数类似文章的研究还是WES为主，但是主流的声响一直在推广WGS<br></li><li>而我的课题就是属于没有被好好设计的那种，那么我的研究对象主要是（全是）WGS的数据了，既然这已经改变不了了，那就好好分析，以找出差异基因，争取早日脱离苦海<br></li><li>那么之后我要分析的内容其实是以WGS的数据为对象，当然，这并没有太大影响，因为很多分析是大同小异的<br></li><li>这里贴一张某测序公司培训时给的WGS数据分析流程，我的样本测序就是交给这家公司来完成的，这张照片比较早，大概是2018年4月时候的培训，不知道这家公司现在还是不是这么分析的，但这并不重要，目前大体的分析流程大同小异，一般发表的文章也会提供分析方法，也是值得大家借鉴的<br></li></ul><p><img src="/2019/06/17/WGS分析笔记（0）- 我的数据/%E5%88%86%E6%9E%90%E6%B5%81%E7%A8%8B.png" alt="分析流程"></p><ul><li>而我后面的分析方法和流程与这里也是有差别的，但我为什么没有贴我自己的流程图呢，主要是因为我的流程细节部分还在不断的修改过程中，我后面的文章会按照我的流程进行一步步的介绍，也会抛出我遇到的问题，希望能得到大家的指点，大家共同交流，批评指正！！！</li></ul>]]></content>
      
      
      <categories>
          
          <category> 我的WGS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WGS </tag>
            
            <tag> 生信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我搭了一个自己的主页</title>
      <link href="/2019/06/17/%E6%88%91%E6%90%AD%E4%BA%86%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%BB%E9%A1%B5/"/>
      <url>/2019/06/17/%E6%88%91%E6%90%AD%E4%BA%86%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84%E4%B8%BB%E9%A1%B5/</url>
      
        <content type="html"><![CDATA[<ul><li>2019年6月17日，天气还是挺热的，我在废了几小时搭建完这个hexo主页以后，决定开始把我的简书文章搬过来了<br></li><li>其实我从来没有写这种东西的习惯，在一个朋友的建议下，我开始写，想把我学到的做过的东西记下来，一方面方便以后自己查阅，一方面，如果能帮助到其他人也是好的，毕竟我在学习之初，还是很痛苦的。<br></li><li>我本身是比较懒的一个人，所以我一开始选择简书，一个直接往上写东西就行的平台，毕竟对我来说，搭建一个主页好累啊，但是简书实在太过分了，莫名其面锁了我的文章，在我申诉以后还不搭理我，这我就很气了，我的系列文章，你中间给我锁一篇算怎么回事！！！<br></li><li>三次申诉无果后，我打算自己建立一个主页，毕竟写技术文章这种是我个人觉得是纯粹找diss的，因为我的技术也就那样，所以装乎我是不会去，而这个主页有个好处是，我没开通评论模块（实在是太麻烦了），这样大家也diss不了我了，但是如果我的文章出现错误，大家完全可以邮件联系我(<a href="mailto:shiyuant@outlook.com" target="_blank" rel="noopener">shiyuant@outlook.com</a>)，我会第一时间答复并且修改<br></li><li>希望能和大家多多交流，共同学习进步<br></li></ul>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
